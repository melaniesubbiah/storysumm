{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:14:00.246763Z",
     "start_time": "2024-08-21T16:13:59.387610Z"
    }
   },
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.metrics import cohen_kappa_score, balanced_accuracy_score, recall_score, precision_score\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:14:01.137574Z",
     "start_time": "2024-08-21T16:14:01.130997Z"
    }
   },
   "source": [
    "ss_file = ('storysumm.json')\n",
    "\n",
    "with open(ss_file, 'r') as f:\n",
    "    storysumm = json.loads(f.read())\n",
    "    \n",
    "print(storysumm[list(storysumm.keys())[0]].keys())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'difficulty', 'story', 'summary', 'errors', 'story-id', 'explanations', 'claims', 'split', 'model'])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:14:02.231674Z",
     "start_time": "2024-08-21T16:14:02.226976Z"
    }
   },
   "source": [
    "splits = {'val': [], 'test': [], 'easy': [], 'hard': [], 'all': []}\n",
    "summary_to_key = {}\n",
    "\n",
    "for id, ss in storysumm.items():\n",
    "    splits['all'].append(ss)\n",
    "    ss['id'] = id\n",
    "    summary_to_key[' '.join(ss['summary']).strip()] = id\n",
    "    if ss['split'] == 'val':\n",
    "        splits['val'].append(ss)\n",
    "    else:\n",
    "        splits['test'].append(ss)\n",
    "    if ss['difficulty'] == 'easy':\n",
    "        splits['easy'].append(ss)\n",
    "    elif ss['difficulty'] == 'hard':\n",
    "        splits['hard'].append(ss)\n",
    "        \n",
    "def word_count(split, text):\n",
    "    wcs = []\n",
    "    for ss in split:\n",
    "        if text == 'summary':\n",
    "            temp = ' '.join(ss[text])\n",
    "        else:\n",
    "            temp = ss[text]\n",
    "        wcs.append(len(nltk.word_tokenize(temp)))\n",
    "    return round(np.mean(wcs))\n",
    "\n",
    "def f_percent(split):\n",
    "    labels = []\n",
    "    for ss in split:\n",
    "        labels.append(int(ss['label']))\n",
    "    return round(np.mean(labels)*100, 1)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:15:35.419309Z",
     "start_time": "2024-08-21T16:15:34.706306Z"
    }
   },
   "source": [
    "print(', '.join([\"split\", \"count\", \"story len.\", \"summary len.\", \"% faith.\"]))\n",
    "print('------------------------------------------------')\n",
    "for split, split_data in splits.items():\n",
    "    result = [len(split_data)]\n",
    "    result.append(word_count(split_data, 'story'))\n",
    "    result.append(word_count(split_data, 'summary'))\n",
    "    result.append(f_percent(split_data))\n",
    "    print(split, result) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split, count, story len., summary len., % faith.\n",
      "------------------------------------------------\n",
      "val [33, 605, 120, 24.2]\n",
      "test [63, 844, 149, 44.4]\n",
      "easy [20, 755, 113, 0.0]\n",
      "hard [40, 707, 141, 0.0]\n",
      "all [96, 762, 139, 37.5]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:15:40.465332Z",
     "start_time": "2024-08-21T16:15:40.460884Z"
    }
   },
   "source": [
    "def regular_load(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        labels = {}\n",
    "        for key, val in data.items():\n",
    "            label_key = 'label' if 'label' in val else 'probs'\n",
    "            labels[key] = val[label_key]\n",
    "    return labels"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:15:40.945690Z",
     "start_time": "2024-08-21T16:15:40.938779Z"
    }
   },
   "source": [
    "alignscore_data = regular_load('evaluators/predicted_labels/alignscore-roberta-large.json')\n",
    "\n",
    "unieval_data = regular_load('evaluators/predicted_labels/unieval.json')\n",
    "    \n",
    "minicheck_data = regular_load('evaluators/predicted_labels/minicheck-flan-t5-large.json')\n",
    "            \n",
    "fables_data = regular_load('evaluators/predicted_labels/fables-gpt-4-turbo-preview.json')\n",
    "        \n",
    "model_methods = {}\n",
    "models = ['claude-3-opus-20240229', 'gpt-4-0125-preview', 'mixtral']\n",
    "modes = ['justquestion', 'cot']\n",
    "for mode in modes:\n",
    "    for model in models:\n",
    "        model_methods[f'{mode}-{model}'] = regular_load(f'evaluators/predicted_labels/{model}/{mode}.json')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:15:41.691618Z",
     "start_time": "2024-08-21T16:15:41.688297Z"
    }
   },
   "source": [
    "def tune_thresh(vals, num=150):\n",
    "    tuning, gvals = [], []\n",
    "    for gd in splits['val']:\n",
    "        tuning.append(vals[gd['id']])\n",
    "        gvals.append(int(gd['label']))\n",
    "    thresholds = np.linspace(0, 1, num)\n",
    "    max_score, best_thresh = 0, 0\n",
    "    for thresh in thresholds:\n",
    "        pred_labels = [1 if x >= thresh else 0 for x in tuning]\n",
    "        score = balanced_accuracy_score(gvals, pred_labels)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_thresh = thresh\n",
    "    return best_thresh"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:20:21.777885Z",
     "start_time": "2024-08-21T16:20:21.773282Z"
    }
   },
   "source": [
    "def table_results(vals, source = None, threshold=None):\n",
    "    output, difficulty, g_labels, p_labels = [], [], [], []\n",
    "    for ss in splits['all']:\n",
    "        if source is None or ss['split'] == source:\n",
    "            difficulty.append(ss['difficulty'])\n",
    "            g_labels.append(int(ss['label']))\n",
    "            p_labels.append(vals[ss['id']])\n",
    "    if threshold:\n",
    "        p_labels = p_labels > threshold\n",
    "    output.append(round(cohen_kappa_score(g_labels, p_labels), 2))\n",
    "    output.append(round(np.mean(p_labels)*100))\n",
    "    output.append(round(precision_score(g_labels, p_labels), 2))\n",
    "    output.append(round(recall_score(g_labels, p_labels), 2))\n",
    "    output.append(100-round(np.mean([p_labels[i] for i in range(len(difficulty)) if difficulty[i] == 'easy'])*100, 1))\n",
    "    output.append(100-round(np.mean([p_labels[i] for i in range(len(difficulty)) if difficulty[i] == 'hard'])*100, 1))\n",
    "    output.append(round(balanced_accuracy_score(g_labels, p_labels)*100, 1))\n",
    "    return \" | \".join([str(x) for x in output]), threshold"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T16:20:22.384209Z",
     "start_time": "2024-08-21T16:20:22.174560Z"
    }
   },
   "source": [
    "print(', '.join(['method', 'Coh. k', '% faith.', 'prec.', 'rec.', '% easy', '% hard', 'bal. acc.', 'threshold' ]))\n",
    "print('------------------------------------------------------------------------')\n",
    "for key, val in model_methods.items():\n",
    "    print(key, table_results(val))\n",
    "print('-----------------')\n",
    "print('fables', table_results(fables_data))\n",
    "print('minicheck', table_results(minicheck_data))\n",
    "print('unieval', table_results(unieval_data, threshold=tune_thresh(unieval_data), source='val'))\n",
    "print('unieval', table_results(unieval_data, threshold=tune_thresh(unieval_data), source='test'))\n",
    "print('alignscore', table_results(alignscore_data, threshold=tune_thresh(alignscore_data), source='val'))\n",
    "print('alignscore', table_results(alignscore_data, threshold=tune_thresh(alignscore_data), source='test'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method, Coh. k, % faith., prec., rec., % easy, % hard, bal. acc., threshold\n",
      "------------------------------------------------------------------------\n",
      "justquestion-claude-3-opus-20240229 ('0.06 | 95 | 0.4 | 1.0 | 20.0 | 2.5 | 54.2', None)\n",
      "justquestion-gpt-4-0125-preview ('0.11 | 70 | 0.42 | 0.78 | 55.0 | 25.0 | 56.4', None)\n",
      "justquestion-mixtral ('0.12 | 91 | 0.41 | 1.0 | 15.0 | 15.0 | 57.5', None)\n",
      "cot-claude-3-opus-20240229 ('0.1 | 90 | 0.41 | 0.97 | 25.0 | 10.0 | 56.1', None)\n",
      "cot-gpt-4-0125-preview ('0.08 | 94 | 0.4 | 1.0 | 25.0 | 2.5 | 55.0', None)\n",
      "cot-mixtral ('0.04 | 97 | 0.39 | 1.0 | 0.0 | 7.5 | 52.5', None)\n",
      "-----------------\n",
      "fables ('0.33 | 55 | 0.53 | 0.78 | 70.0 | 52.5 | 68.1', None)\n",
      "minicheck ('0.02 | 16 | 0.4 | 0.17 | 90.0 | 82.5 | 50.8', None)\n",
      "unieval ('0.25 | 39 | 0.38 | 0.62 | 80.0 | 60.0 | 65.3', 0.8791946308724832)\n",
      "unieval ('0.04 | 30 | 0.47 | 0.32 | 80.0 | 68.0 | 51.8', 0.8791946308724832)\n",
      "alignscore ('0.21 | 42 | 0.36 | 0.62 | 80.0 | 53.3 | 63.3', 0.785234899328859)\n",
      "alignscore ('-0.07 | 68 | 0.42 | 0.64 | 40.0 | 24.0 | 46.4', 0.785234899328859)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T10:49:27.573385Z",
     "start_time": "2024-08-21T10:49:27.569645Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
